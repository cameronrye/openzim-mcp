name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * 1'  # Weekly on Mondays at 2 AM UTC

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write  # For commenting on PRs

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for comparison

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"

    - name: Set up Python
      run: uv python install 3.12

    - name: Install dependencies
      run: |
        uv sync
        uv run pip install pytest-benchmark

    - name: Download ZIM test data
      run: |
        make download-test-data

    - name: Run performance benchmarks
      run: |
        # Run benchmarks and save results
        uv run pytest tests/ -k "benchmark" --benchmark-json=benchmark-results.json --benchmark-only || true
        
        # If no benchmark tests exist yet, create a placeholder
        if [ ! -f benchmark-results.json ]; then
          echo '{"benchmarks": [], "datetime": "'$(date -Iseconds)'", "version": "placeholder"}' > benchmark-results.json
        fi

    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      continue-on-error: true  # Don't fail if gh-pages doesn't exist yet
      with:
        tool: 'pytest'
        output-file-path: benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        alert-threshold: '150%'  # Alert if performance degrades by 50%
        fail-on-alert: false
        # Store benchmarks in a subdirectory to avoid conflicts with main site
        gh-pages-branch: 'gh-pages'
        benchmark-data-dir-path: 'benchmarks'

    - name: Compare benchmark results (PR)
      uses: benchmark-action/github-action-benchmark@v1
      if: github.event_name == 'pull_request'
      with:
        tool: 'pytest'
        output-file-path: benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        comment-on-alert: true
        alert-threshold: '150%'
        fail-on-alert: false

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark-results.json

  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"

    - name: Set up Python
      run: uv python install 3.12

    - name: Install dependencies
      run: |
        uv sync
        uv run pip install memory-profiler psutil

    - name: Download ZIM test data
      run: |
        make download-test-data

    - name: Run memory profiling
      run: |
        # Create a simple memory profiling script
        cat > memory_profile.py << 'EOF'
        import os
        import psutil
        import time
        from openzim_mcp.main import main
        
        def profile_memory():
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            print(f"Initial memory: {initial_memory:.2f} MB")
            
            # Simulate some operations
            # This would need to be expanded with actual benchmark operations
            time.sleep(1)
            
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            print(f"Final memory: {final_memory:.2f} MB")
            print(f"Memory increase: {final_memory - initial_memory:.2f} MB")
            
            return {
                'initial_memory_mb': initial_memory,
                'final_memory_mb': final_memory,
                'memory_increase_mb': final_memory - initial_memory
            }
        
        if __name__ == "__main__":
            results = profile_memory()
            print(f"Memory profiling results: {results}")
        EOF
        
        uv run python memory_profile.py > memory-profile-results.txt

    - name: Upload memory profiling results
      uses: actions/upload-artifact@v4
      with:
        name: memory-profile-results
        path: memory-profile-results.txt
